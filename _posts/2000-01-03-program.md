---
title: "program"
bg: orange
color: white
style: left
fa-icon: calendar
---

<h3 id="papers">Program</h3>

<table id="xxtable">
<tr id="xxtr"><td id="xxtd" colspan="2"><p id="xxhead1">Tuesday, December 10th</p></td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">13:30 13:35</p> </td><td id="xxtd"> <p id="xxbold">Opening session (Marco Netto, IBM Research)</p></td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">13:35 14:30</p> </td><td id="xxtd"> <p id="xxbold"><a href="https://hpml2019.github.io/#keynote">KEYNOTE - TBD</a></p><p id="xxit">Valerie Issarny</p><p id="xxp">Inria Paris</p></td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">14:30 15:00</p> </td><td id="xxtd"> <p id="xxbold">Adaptive Mediation for Data Exchange in IoT Systems</p><p id="xxit">Andrew Chio, Georgios Bouloukakis, Cheng-Hsin Hsu, Sharad Mehrotra, Nalini Venkatasubramanian </p><p id="xxp">University of California (Irvine), National Tsing Hua University</p> <p id="xxp"><a id="talk0btn" class="xxa">[More information]</a></p> </td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">15:00 15:30</p> </td><td id="xxtd"><center> <p id="xxp">Coffee Break</p></center></td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">15:30 16:00</p> </td><td id="xxtd"> <p id="xxbold">Service Resilience Framework for Enhanced End-to-End Service Quality</p><p id="xxit"> Dhanya R Mathews, Lakshmi Jagarlamudi </p><p id="xxp">Indian Institute of Science</p> <p id="xxp"><a id="talk1btn" class="xxa">[More information]</a></p> </td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">16:00 16:30</p> </td><td id="xxtd"> <p id="xxbold">AFIrM: An Adaptive Functional Middleware</p><p id="xxit">Andre Silva, Nelson Rosa </p><p id="xxp">State University of Rio Grande do Norte, Federal University of Pernambuco</p> <p id="xxp"><a id="talk2btn" class="xxa">[More information]</a></p> </td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">16:30 17:30</p> </td><td id="xxtd"> <p id="xxbold">Panel - TBD</p><p id="xxit"></p><p id="xxp"></p> <p id="xxp"><a id="talk3btn" class="xxa">[More information]</a></p> </td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">17:30 17:35</p> </td><td id="xxtd"><p id="xxbold">Closing remarks</p></td></tr>

</table>

<div id="talk0" class="modal">
  <div class="modal-content">
    <span id="close0" class="close">&times;</span>
    <p class="xxblack" id="xxbold">Performance Optimization on Model Synchronization in Parallel Stochastic Gradient Descent Based SVM</p>
    <p class="xxblack" id="xxit">Vibhatha Abeykoon, Geoffrey Fox and Minje Kim</p>
    <p class="xxblack" id="xxp">Indiana University</p>
    &nbsp;
    <p class="xxblack" id="xxit"><a href="https://arxiv.org/abs/1905.01219" style="color:blue">Authors' preprint on arXiv</a></p>
    <p class="xxblack" id="xxbold"><a href="slides/HPML-PSGDSVM.pptx.pdf" style="color:blue">slides</a></p>
    <p class="xxblack" id="xxp">Understanding the bottlenecks in implementing stochastic gradient descent (SGD)-based distributed support vector machines (SVM) algorithm is important in training larger data sets. The communication time to do the model synchronization across the parallel processes is the main bottleneck that causes inefficiency in the training process. The model synchronization is directly affected by the mini-batch size of data processed before the global synchronization. In producing an efficient distributed model, the communication time in training model synchronization has to be as minimum as possible while retaining a high testing accuracy. The effect from model synchronization frequency over the convergence of the algorithm and accuracy of the generated model must be well understood to design an efficient distributed model. In this research, we identify the bottlenecks in model synchronization in parallel stochastic gradient descent (PSGD)-based SVM algorithm with respect to the training model synchronization frequency (MSF). Our research shows that by optimizing the MSF in the data sets that we used, a reduction of 98\% in communication time can be gained (16x - 24x speed up) with respect to high-frequency model synchronization. The training model optimization discussed in this paper guarantees a higher accuracy than the sequential algorithm along with faster convergence.</p>
  </div>
</div>
<div id="talk1" class="modal">
  <div class="modal-content">
    <span id="close1" class="close">&times;</span>
    <p class="xxblack" id="xxbold">Distributed MCMC Inference in Dirichlet Process Mixture Models Using Julia</p>
    <p class="xxblack" id="xxit">Or Dinari, Angel Yu, Oren Freifeld, John Fisher</p>
    <p class="xxblack" id="xxp">Ben-Gurion University of the Negev, MIT</p>
    &nbsp;
    <p class="xxblack" id="xxbold"><a href="slides/talk.pdf" style="color:blue">slides</a></p>
    <p class="xxblack" id="xxp">With the advent of large data sets, the need for general-purpose massively-parallel analysis tools become ever greater. In unsupervised learning, Bayesian nonparametric mixture models, exemplified by the Dirichlet-Process Mixture Model (DPMM), provide a principled Bayesian approach while adapting model complexity to the data. Despite their potential, however, DPMM's have yet to become a popular tool. This is partly due to the lack of friendly software tools that can handle large datasets efficiently. Here we show how, using Julia, one can achieve efficient and easily-modifiable implementation of distributed inference in DPMMs. Particularly, we show how a recent parallel MCMC inference algorithm -- originally implemented in C++ for a single multi-core machine -- can be distributed efficiently across multiple multi-core machines using a distributed-memory model. This leads to speedups, alleviates memory and storage limitations, and lets us learn DPMMs from significantly larger datasets and of higher dimensionality. It also turned out that even on a single machine our Julia implementation handles higher dimensions more gracefully (at least for Gaussians) than the original C++ implementation. Finally, we use our implementation to learn a model of image patches and apply the learned model for image denoising.</p>
  </div>
</div>
<div id="talk2" class="modal">
  <div class="modal-content">
    <span id="close2" class="close">&times;</span>
    <p class="xxblack" id="xxbold">TensorFlow on state-of-the-art HPC clusters: a machine learning use case</p>
    <p class="xxblack" id="xxit">Guillem Ramirez-Gargallo, Marta Garcia-Gasulla, Filippo Mantovani</p>
    <p class="xxblack" id="xxp">Barcelona Supercomputing Center</p>
    &nbsp;
    <p class="xxblack" id="xxbold"><a href="slides/HPML19_TF_on_HPC_clusters_20190514_0828.pdf" style="color:blue">slides</a></p>
    <p class="xxblack" id="xxp">The recent rapid growth of the data-flow programming paradigm enabled the development of specific architectures, e.g., for machine learning. The most known example is the Tensor Processing Unit (TPU) by Google. Standard data-centers, however, still can not foresee large partitions dedicated to machine learning specific architectures. Within data-centers, the High-Performance Computing (HPC) clusters are highly parallel machines targeting a broad class of compute-intensive workflows, as such they can be used for tackling machine learning challenges. On top of this, HPC architectures are rapidly changing, including accelerators and instruction sets other than the classical x86 CPUs. In this blurry scenario, identifying which are the best hardware/software configurations to efficiently support machine learning workloads on HPC clusters is not trivial. In this paper, we considered the workflow of TensorFlow for image recognition. We highlight the strong dependency of the performance in the training phase on the availability of arithmetic libraries optimized for the underlying architecture. Following the example of Intel leveraging the MKL libraries for improving the TensorFlow performance, we plugged the Arm Performance Libraries into TensorFlow and tested on an HPC cluster based on Marvell ThunderX2 CPUs. Also, we performed a scalability study on three state-of-the-art HPC clusters based on different CPU architectures, x86 Intel Skylake, Arm-v8 Marvell ThunderX2, and PowerPC IBM Power9.</p>
  </div>
</div>

<script>
var pnum = 7
var modal = []
var btn = []
var span = []
var b2t = []
var c2t = []
for (i = 0; i < pnum; i++) {
    modal[i] = document.getElementById('talk' + i);
    btn[i] = document.getElementById('talk' + i + "btn");
    span[i] = document.getElementById("close" + i);
    b2t['talk' + i + 'btn'] = i;
    c2t['close' + i] = i;
}

// When the user clicks the button, open the modal 
for (k = 0; k < pnum; k++) {
    btn[k].onclick = function() {
        modal[b2t[this.id]].style.display = "block"; 
    }
}

// When the user clicks on <span> (x), close the modal
for (i = 0; i < pnum; i++) {
    span[i].onclick = function() {
        modal[c2t[this.id]].style.display = "none";
    }
}

// When the user clicks anywhere outside of the modal, close it
window.onclick = function(event) {
    for (i = 0; i < pnum; i++) {
        if (event.target == modal[i]) {
            modal[i].style.display = "none";
        }
    }
}
</script>
